# COMPREHENSIVE MULTI-PROVIDER MIGRATION GUIDE
# Study Buddy: From Single Groq Provider to Multi-Provider Support

This guide provides step-by-step instructions to migrate your Study Buddy application from using only Groq to supporting multiple LLM providers (Groq, OpenAI, and future providers). Each step includes specific file locations, line numbers, before/after code, and detailed explanations.

## CURRENT STATE ANALYSIS
=======================

Your application currently has:
- ✅ Provider factory already created (`src/llm/provider_factory.py`)
- ✅ UI provider selection already implemented (`application.py` lines 47-50)
- ❌ QuestionGenerator still using direct Groq import
- ❌ Settings not configured for multi-provider support
- ❌ Missing OpenAI dependencies in requirements.txt

## PHASE 1: DEPENDENCY UPDATES
=============================

### STEP 1: Update Requirements File
**File:** `requirements.txt`
**Lines:** 1-5

**WHY:** We need to add OpenAI dependencies while keeping Groq for backward compatibility.

**BEFORE:**
```
langchain
langchain-groq
pandas
streamlit
python-dotenv
```

**AFTER:**
```
langchain
langchain-groq
langchain-openai
openai
pandas
streamlit
python-dotenv
```

**EXPLANATION:** 
- Keep `langchain-groq` for existing Groq functionality
- Add `langchain-openai` for OpenAI integration
- Add `openai` for direct OpenAI API access if needed
- This allows users to use either provider without breaking existing functionality

---

## PHASE 2: CONFIGURATION UPDATES
=================================

### STEP 2: Update Settings Configuration
**File:** `src/config/settings.py`
**Lines:** 1-13

**WHY:** We need to add support for multiple providers, default provider selection, and provider-specific model configurations.

**BEFORE:**
```python
import os
from dotenv import load_dotenv

load_dotenv()

class Settings():

    GROK_API_KEY = os.getenv("GROQ_API_KEY")
    MODEL_NAME = "llama-3.1-8b-instant"
    TEMPERATURE = 0.9
    MAX_RETRIES = 3

settings = Settings()
```

**AFTER:**
```python
import os
from dotenv import load_dotenv

load_dotenv()

class Settings():
    # API Keys for different providers
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    # Default provider (can be overridden by user selection)
    DEFAULT_PROVIDER = os.getenv("DEFAULT_PROVIDER", "groq")
    
    # Provider-specific default models
    DEFAULT_MODELS = {
        "groq": "llama-3.1-8b-instant",
        "openai": "gpt-3.5-turbo"
    }
    
    # Global settings
    TEMPERATURE = 0.9
    MAX_RETRIES = 3

settings = Settings()
```

**EXPLANATION:**
- Added `OPENAI_API_KEY` for OpenAI provider support
- Added `DEFAULT_PROVIDER` to set which provider to use by default
- Added `DEFAULT_MODELS` dictionary to map each provider to its default model
- This allows the system to automatically select appropriate models for each provider
- Users can override the default provider through environment variables

---

## PHASE 3: BACKEND INTEGRATION
==============================

### STEP 3: Update Question Generator
**File:** `src/generator/question_generator.py`
**Lines:** 1-13

**WHY:** The QuestionGenerator is currently hardcoded to use Groq. We need to make it provider-agnostic so it can work with any provider selected by the user.

**BEFORE:**
```python
from langchain.output_parsers import PydanticOutputParser
from src.models.question_schemas import MCQQuestion, FillBlankQuestion
from src.prompts.templates import get_mcq_prompt_template, get_fill_blank_prompt_template
from src.llm.groq_client import get_groq_llm
from src.config.settings import settings
from src.common.logger import get_logger
from src.common.custom_exception import CustomException


class QuestionGenerator:
    def __init__(self):
        self.llm = get_groq_llm()
        self.logger = get_logger(self.__class__.__name__)
```

**AFTER:**
```python
from langchain.output_parsers import PydanticOutputParser
from src.models.question_schemas import MCQQuestion, FillBlankQuestion
from src.prompts.templates import get_mcq_prompt_template, get_fill_blank_prompt_template
from src.llm.provider_factory import get_llm
from src.config.settings import settings
from src.common.logger import get_logger
from src.common.custom_exception import CustomException


class QuestionGenerator:
    def __init__(self, provider: str = None, model: str = None, temperature: float = None):
        self.llm = get_llm(provider=provider, model=model, temperature=temperature)
        self.logger = get_logger(self.__class__.__name__)
```

**EXPLANATION:**
- Replaced direct Groq import with provider factory import
- Modified constructor to accept provider, model, and temperature parameters
- The `get_llm()` function will automatically select the appropriate LLM based on the provider
- This makes the QuestionGenerator completely provider-agnostic
- If no provider is specified, it will use the default provider from settings

---

## PHASE 4: UI INTEGRATION
=========================

### STEP 4: Update Application to Pass Provider Selection
**File:** `application.py`
**Lines:** 70-79

**WHY:** The UI already has provider selection, but the QuestionGenerator is not receiving the selected provider. We need to pass the user's provider choice to the generator.

**BEFORE:**
```python
if st.sidebar.button("Generate Quiz") and 'exam' in locals() and 'subject' in locals() and 'topic' in locals():
    st.session_state.quiz_submitted = False
    generator = QuestionGenerator()
    success = st.session_state.quiz_manager.generate_questions(
        generator,
        exam, subject, topic, question_type, difficulty, num_questions
    )
```

**AFTER:**
```python
if st.sidebar.button("Generate Quiz") and 'exam' in locals() and 'subject' in locals() and 'topic' in locals():
    st.session_state.quiz_submitted = False
    generator = QuestionGenerator(provider=provider)
    success = st.session_state.quiz_manager.generate_questions(
        generator,
        exam, subject, topic, question_type, difficulty, num_questions
    )
```

**EXPLANATION:**
- Added `provider=provider` parameter when creating QuestionGenerator
- This passes the user's provider selection from the sidebar to the generator
- The generator will now use the selected provider instead of defaulting to Groq
- This connects the UI provider selection to the actual LLM instantiation

---

## PHASE 5: ENVIRONMENT CONFIGURATION
====================================

### STEP 5: Create Environment Configuration Template
**File:** `.env.example` (new file)

**WHY:** Users need to know which environment variables to set for different providers.

**CONTENT:**
```bash
# LLM Provider Configuration
DEFAULT_PROVIDER=groq

# Groq Configuration
GROQ_API_KEY=your_groq_api_key_here

# OpenAI Configuration  
OPENAI_API_KEY=your_openai_api_key_here
```

**EXPLANATION:**
- Provides a template for users to configure their environment
- Shows which API keys are needed for each provider
- Allows users to set a default provider preference
- Users can copy this to `.env` and fill in their actual API keys

---

## PHASE 6: OPTIONAL ENHANCEMENTS
=================================

### STEP 6: Add Model Selection per Provider (Optional)
**File:** `application.py`
**Lines:** 47-50 (after provider selection)

**WHY:** Different providers have different models available. Users should be able to select specific models.

**ADD AFTER LINE 50:**
```python
# Model selection based on provider
if provider == "groq":
    model = st.sidebar.selectbox("Model", 
                                ["llama-3.1-8b-instant", "llama-3.1-70b-versatile", "mixtral-8x7b-32768"],
                                index=0)
elif provider == "openai":
    model = st.sidebar.selectbox("Model",
                                ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
                                index=0)
else:
    model = None
```

**THEN UPDATE LINE 72:**
```python
generator = QuestionGenerator(provider=provider, model=model)
```

**EXPLANATION:**
- Provides provider-specific model selection
- Users can choose the best model for their needs and budget
- Different models have different capabilities and costs
- Falls back to default model if no selection is made

---

## PHASE 7: ERROR HANDLING IMPROVEMENTS
======================================

### STEP 7: Add Provider Validation (Optional)
**File:** `application.py`
**Lines:** 70-79 (around the generate button)

**WHY:** We should validate that the selected provider is properly configured before attempting to generate questions.

**ADD BEFORE LINE 70:**
```python
# Validate provider configuration
def validate_provider(provider):
    if provider == "groq" and not settings.GROQ_API_KEY:
        st.sidebar.error("Groq API key not configured. Please set GROQ_API_KEY in your environment.")
        return False
    elif provider == "openai" and not settings.OPENAI_API_KEY:
        st.sidebar.error("OpenAI API key not configured. Please set OPENAI_API_KEY in your environment.")
        return False
    return True
```

**THEN UPDATE THE BUTTON CONDITION:**
```python
if st.sidebar.button("Generate Quiz") and 'exam' in locals() and 'subject' in locals() and 'topic' in locals() and validate_provider(provider):
```

**EXPLANATION:**
- Prevents users from attempting to generate questions with unconfigured providers
- Provides clear error messages about missing API keys
- Improves user experience by catching configuration issues early

---

## TESTING INSTRUCTIONS
======================

### Test Scenario 1: Groq Provider
1. Set `GROQ_API_KEY` in your environment
2. Select "groq" as provider in the UI
3. Generate a quiz and verify it works as before

### Test Scenario 2: OpenAI Provider  
1. Set `OPENAI_API_KEY` in your environment
2. Select "openai" as provider in the UI
3. Generate a quiz and verify it works with OpenAI

### Test Scenario 3: Provider Switching
1. Generate a quiz with Groq
2. Switch to OpenAI provider
3. Generate another quiz
4. Verify both work correctly

---

## MIGRATION CHECKLIST
=====================

- [ ] Update `requirements.txt` with OpenAI dependencies
- [ ] Update `src/config/settings.py` with multi-provider configuration
- [ ] Update `src/generator/question_generator.py` to use provider factory
- [ ] Update `application.py` to pass provider selection to generator
- [ ] Create `.env.example` template file
- [ ] Test with Groq provider (existing functionality)
- [ ] Test with OpenAI provider (new functionality)
- [ ] Test provider switching functionality
- [ ] Verify error handling for missing API keys

---

## BENEFITS OF THIS MIGRATION
============================

1. **Flexibility**: Users can choose between different LLM providers
2. **Cost Optimization**: Users can select cheaper models when appropriate
3. **Reliability**: If one provider is down, users can switch to another
4. **Future-Proof**: Easy to add new providers (Anthropic, Cohere, etc.)
5. **Backward Compatibility**: Existing Groq functionality remains unchanged
6. **User Choice**: Users can select models based on their specific needs

---

## FUTURE ENHANCEMENTS
=====================

After completing this migration, you can easily add:

1. **More Providers**: Anthropic Claude, Cohere, etc.
2. **Model Comparison**: Side-by-side comparison of different models
3. **Cost Tracking**: Track API usage and costs per provider
4. **Performance Metrics**: Compare response times and quality
5. **Provider-Specific Features**: Leverage unique capabilities of each provider

This migration maintains all existing functionality while adding powerful multi-provider support that will serve your users better and make your application more robust and flexible.

